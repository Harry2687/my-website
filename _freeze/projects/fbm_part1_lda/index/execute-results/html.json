{
  "hash": "c8a979388bc6e0b9ed7e2da8accbaa05",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Facebook Messenger Data Part 1: Latent Dirichlet Allocation\"\ndescription: \"Identification and classification of topics in Facebook Messenger conversations using Latent Dirichlet Allocation (LDA).\"\ncategories: [LDA, Python]\nauthor:\n  name: Harry Zhong\ndate: today\n# image: \n---\n\n# Introduction\n\nAfter analysing my Spotify data, I thought it would be interesting to investigate another dataset generated by my online activity: Facebook Messenger chat logs. However, since chat logs are composed of text data we'll have to learn some basic natural language processing techniques to gain any meaningful insights from this dataset.\n\nThis is part 1 of 2 of my analysis on my Facebook Messenger conversations, where I'll use latent Dirichlet allocation to (hopefully) identify topics within conversations.\n\n# Data Extraction\n\nFirst we'll need to load the data, which Facebook provides as a series of `json` files. The raw data is structured like the sample below.\n\n::: {#06f3b9c3 .cell execution_count=1}\n``` {.python .cell-code}\nwith open('sample_messages.json') as file:\n    sample = file.read()\n\nprint(sample)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{\n    \"participants\": [\n      {\n        \"name\": \"Alice\"\n      },\n      {\n        \"name\": \"Bob\"\n      }\n    ],\n    \"messages\": [\n      {\n        \"sender_name\": \"Alice\",\n        \"timestamp_ms\": 1710762618535,\n        \"content\": \"Hey, Bob! Did you see the new movie?\",\n        \"is_geoblocked_for_viewer\": false\n      },\n      {\n        \"sender_name\": \"Bob\",\n        \"timestamp_ms\": 1710762591958,\n        \"content\": \"Yeah, I just watched it last night. What did you think of it?\",\n        \"is_geoblocked_for_viewer\": false\n      },\n      {\n        \"sender_name\": \"Bob\",\n        \"timestamp_ms\": 1710762583886,\n        \"content\": \"I loved the plot twist at the end. It was so unexpected!\",\n        \"is_geoblocked_for_viewer\": false\n      },\n      {\n        \"sender_name\": \"Alice\",\n        \"timestamp_ms\": 1710749703948,\n        \"content\": \"I know, right? I was totally surprised when it happened.\",\n        \"is_geoblocked_for_viewer\": false\n      },\n      {\n        \"sender_name\": \"Bob\",\n        \"timestamp_ms\": 1710749659963,\n        \"content\": \"What did you think of the special effects?\",\n        \"reactions\": [\n          {\n            \"reaction\": \"\\ud83d\\ude04\",\n            \"actor\": \"Alice\"\n          }\n        ],\n        \"is_geoblocked_for_viewer\": false\n      },\n      {\n        \"sender_name\": \"Bob\",\n        \"timestamp_ms\": 1710749648743,\n        \"content\": \"I thought they were really well done. The explosions were so cool!\",\n        \"is_geoblocked_for_viewer\": false\n      },\n      {\n        \"sender_name\": \"Alice\",\n        \"timestamp_ms\": 1710749547622,\n        \"content\": \"Definitely! I loved the action scenes.\",\n        \"is_geoblocked_for_viewer\": false\n      },\n      {\n        \"sender_name\": \"Alice\",\n        \"timestamp_ms\": 1710749516271,\n        \"content\": \"I'm so glad we watched it together. It was a lot more fun that way.\",\n        \"is_geoblocked_for_viewer\": false\n      },\n      {\n        \"sender_name\": \"Bob\",\n        \"timestamp_ms\": 1710749502826,\n        \"content\": \"Definitely! We should do it again soon.\",\n        \"reactions\": [\n          {\n            \"reaction\": \"\\ud83d\\ude04\",\n            \"actor\": \"Alice\"\n          }\n        ],\n        \"is_geoblocked_for_viewer\": false\n      },\n      {\n        \"sender_name\": \"Alice\",\n        \"timestamp_ms\": 1710749462947,\n        \"content\": \"Sounds like a plan to me!\",\n        \"is_geoblocked_for_viewer\": false\n      }\n    ]\n  }\n```\n:::\n:::\n\n\nIdeally though, we'll need to extract the relevant information from each `json` file and store it as a dataframe. In this case only the `messages` property is required. To do this, we'll first import some libraries which we'll need.\n\n::: {#3c311ae9 .cell execution_count=2}\n``` {.python .cell-code}\nimport os\nimport json\nimport pandas as pd\nimport re\nimport gensim as gs\n```\n:::\n\n\nThen, we can define a function which takes a directory containing `json` files as an input and returns a dataframe using the `messages` property of the `json` files in a `pandas` dataframe.\n\n::: {#4c296e99 .cell execution_count=3}\n``` {.python .cell-code}\ndef ms_import_data(directory: str) -> pd.DataFrame:\n    data_file_names = os.listdir(directory)\n    data_files = [os.path.join(directory, data_file_name) for data_file_name in data_file_names]\n\n    messenger_data = pd.DataFrame()\n\n    for data_file in data_files:\n        with open(data_file) as file:\n            data = json.load(file)\n        json_data = pd.json_normalize(data['messages'])\n        messenger_data = pd.concat([messenger_data, json_data])\n\n    messenger_data = (\n        messenger_data[[\n            'sender_name',\n            'timestamp_ms',\n            'content'\n        ]]\n        .dropna()\n        .sort_values('timestamp_ms', ascending=True)\n    )\n\n    messenger_data['timestamp'] = pd.to_datetime(messenger_data['timestamp_ms'], unit='ms')\n\n    messenger_data = messenger_data[[\n        'sender_name',\n        'timestamp',\n        'content'\n    ]]\n\n    return messenger_data\n```\n:::\n\n\nMy analysis will focus on a group chat called \"The Office\" which should have the most data, starting from 2019 to early 2024 when Facebook sent me the data. Using the function we just made, we can import the chat logs and look at the resulting dataframe.\n\n::: {#570ebf4d .cell execution_count=4}\n``` {.python .cell-code}\nchat_data = ms_import_data('data/the_office')\nchat_data.head(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sender_name</th>\n      <th>timestamp</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6328</th>\n      <td>Harry Zhong</td>\n      <td>2019-04-08 11:23:07.519</td>\n      <td>You created the group.</td>\n    </tr>\n    <tr>\n      <th>6327</th>\n      <td>Harry Zhong</td>\n      <td>2019-04-08 11:23:08.454</td>\n      <td>Hoi</td>\n    </tr>\n    <tr>\n      <th>6326</th>\n      <td>Dhruv Jobanputra</td>\n      <td>2019-04-08 11:23:19.718</td>\n      <td>Hoi</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNow that we have the raw data, we can move on to additional cleaning which is required for LDA:\n\n1. Convert all text to lower case.\n2. Remove non-text characters.\n3. Remove messages that aren't part of a conversation (reactions and links).\n4. Remove stop words (words which can belong to any topic, such as conjunctions and abbreviations).\n5. Group messages into conversations which will be used as documents for LDA.\n\nPoints 1 to 3 are fairly straightforward to implement using `pandas`. However, point 4 brings us to our first usage of `gensim`, a Python topic modelling library, where it's used to remove generic stop words from the chat log. Additional stop words specific to the chat are then defined and removed on top of the words from `gensim`.\n\nThe code used to implement points 1 to 4 is shown below.\n\n::: {#3e919b95 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Preprocessing Code\"}\ndef remove_custom_stopwords(document: str, stopwords: list) -> str:\n    for word in stopwords:\n        pattern = r'\\b'+word+r'\\b'\n        document = re.sub(pattern, '', document).replace('  ', ' ')\n    \n    return document\n\ndef lda_preprocess(dataframe: pd.DataFrame, content_col: str, cleaned_col: str, rm_stopwords: bool=True) -> pd.DataFrame:\n    dataframe[cleaned_col] = (\n        dataframe[content_col]\n        .str.lower()\n        .str.strip()\n        .str.replace('[^a-z\\\\s]', '', regex=True) # remove anything that isn't text or spaces\n        .str.replace('\\\\s{2,}', ' ', regex=True) # replace 2+ spaces with single space\n    )\n\n    chat_actions = [\n        'reacted to your message',\n        'https'\n    ]\n\n    # remove chat actions which aren't part of the conversation\n    dataframe = dataframe[\n        ~dataframe[cleaned_col]\n        .str.contains(\n            '|'.join(chat_actions)\n        )\n    ]\n\n    # stopwords which are not included in gensim\n    custom_stopwords = [\n        'aditya',\n        'gupta',\n        'dhruv',\n        'jobanputra',\n        'harry',\n        'zhong',\n        'mansoor',\n        'khawaja',\n        'saquib',\n        'ahmed',\n        'anand',\n        'karna',\n        'chaitany',\n        'goyal',\n        'himal',\n        'pandey',\n        'anirudth',\n        'sanivarapu',\n        'sai',\n        'roshan',\n        'prashant',\n        'u', \n        'lmao', \n        'lol', \n        'ur', \n        'like', \n        'yea', \n        'thats', \n        'nah', \n        'im', \n        'yeh', \n        'dont',\n        'yeah', \n        'gonna', \n        'didnt',\n        'idk',\n        'got',\n        'r',\n        'sure',\n        'come',\n        'stuff',\n        'k',\n        'damn',\n        'ez',\n        'ill',\n        'smh',\n        'f',\n        'ofc',\n        'u',\n        'tf',\n        'nice',\n        'wtf',\n        'tbf',\n        'ngl',\n        'm'\n    ]\n\n    if rm_stopwords:\n        # remove gensim and custom stopwords\n        dataframe.loc[:, cleaned_col] = (\n            dataframe[cleaned_col]\n            .apply(gs.parsing.preprocessing.remove_stopwords)\n            .apply(remove_custom_stopwords, args=(custom_stopwords,))\n            .str.strip()\n            .str.replace('\\\\s{2,}', ' ', regex=True)\n        )\n\n    return dataframe\n```\n:::\n\n\nWe can again look at the resulting dataframe.\n\n::: {#6ef69f92 .cell execution_count=6}\n``` {.python .cell-code}\nchat_data = lda_preprocess(chat_data, 'content', 'clean_content')\nchat_data.head(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sender_name</th>\n      <th>timestamp</th>\n      <th>content</th>\n      <th>clean_content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6328</th>\n      <td>Harry Zhong</td>\n      <td>2019-04-08 11:23:07.519</td>\n      <td>You created the group.</td>\n      <td>created group</td>\n    </tr>\n    <tr>\n      <th>6327</th>\n      <td>Harry Zhong</td>\n      <td>2019-04-08 11:23:08.454</td>\n      <td>Hoi</td>\n      <td>hoi</td>\n    </tr>\n    <tr>\n      <th>6326</th>\n      <td>Dhruv Jobanputra</td>\n      <td>2019-04-08 11:23:19.718</td>\n      <td>Hoi</td>\n      <td>hoi</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNext we'll move on to separating the chat log into conversations, since LDA assumes our complete body of text is composed of a number of documents. There are a number of ways to split a chat log into sections, such as dividing into sections based on text length. However, ideally each document would have a primary topic which is picked up by LDA. Thus, I settled on grouping messages based on the length of time between each message, if the time between two messages exceeds a certain threshold then those two messages would belong to different conversations. I chose the cutoff to be 10 minutes using the educated guess methodology.\n\nImplementing this logic in `pandas` is fairly simple, one approach is to:\n\n1. Create a column with the difference between the current row and the previous row.\n2. Create a boolean column indicating if the time difference is greater than the cutoff.\n3. Count the cumulative number of `True` rows as the conversation number.\n4. Group by conversation number and concatenate the message content column.\n5. Convert conversations from dataframe to list.\n\nThis approach is implemented in the function below.\n\n::: {#bec9676d .cell execution_count=7}\n``` {.python .cell-code}\ndef lda_getdocs(dataframe: pd.DataFrame, content_col: str, ts_col: str, conv_cutoff: int=600):\n    # calculate difference between each message\n    dataframe['time_diff'] = (\n        dataframe[ts_col]\n        .diff()\n        .fillna(pd.Timedelta(seconds=0))\n    )\n    dataframe['time_diff'] = (\n        dataframe['time_diff']\n        .dt.total_seconds()\n    )\n\n    # group dataframe into different conversations based on cutoff value\n    dataframe['new_conv'] = dataframe['time_diff'] > conv_cutoff\n    dataframe['conv_num'] = 'Conv ' + (\n        dataframe['new_conv']\n        .cumsum()\n        .astype(str)\n    )\n\n    # join together messages in the same coversation\n    conversations = (\n        dataframe\n        .groupby('conv_num')\n        [content_col]\n        .apply(lambda x: ' '.join(map(str, x)))\n        .str.strip()\n        .str.replace('\\\\s{2,}', ' ', regex=True)\n        .reset_index()\n    )\n    conversations = conversations[conversations[content_col] != '']\n\n    documents = conversations[content_col].tolist()\n\n    return documents\n```\n:::\n\n\nWe can then apply the function to our dataset and view the new columns that were created in our dataframe.\n\n::: {#8d968945 .cell execution_count=8}\n``` {.python .cell-code}\ndocuments = lda_getdocs(chat_data, 'clean_content', 'timestamp')\nchat_data.head(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sender_name</th>\n      <th>timestamp</th>\n      <th>content</th>\n      <th>clean_content</th>\n      <th>time_diff</th>\n      <th>new_conv</th>\n      <th>conv_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6328</th>\n      <td>Harry Zhong</td>\n      <td>2019-04-08 11:23:07.519</td>\n      <td>You created the group.</td>\n      <td>created group</td>\n      <td>0.000</td>\n      <td>False</td>\n      <td>Conv 0</td>\n    </tr>\n    <tr>\n      <th>6327</th>\n      <td>Harry Zhong</td>\n      <td>2019-04-08 11:23:08.454</td>\n      <td>Hoi</td>\n      <td>hoi</td>\n      <td>0.935</td>\n      <td>False</td>\n      <td>Conv 0</td>\n    </tr>\n    <tr>\n      <th>6326</th>\n      <td>Dhruv Jobanputra</td>\n      <td>2019-04-08 11:23:19.718</td>\n      <td>Hoi</td>\n      <td>hoi</td>\n      <td>11.264</td>\n      <td>False</td>\n      <td>Conv 0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAnd view a our documents.\n\n::: {#0cb6337c .cell execution_count=9}\n``` {.python .cell-code}\nfor doc in documents[0:3]:\n    print(doc[0:50] + '...')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncreated group hoi hoi premium tech support named g...\nepisodes grand tour season episode season episode ...\ncute dog looks sohails shitzoo pubg visited nepam ...\n```\n:::\n:::\n\n\n# Latent Dirichlet Allocation\n\n## How does LDA work?\n\n## Hyperparameter Tuning\n\n# Results\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}